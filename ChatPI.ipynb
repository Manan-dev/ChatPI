{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatPI'...\n",
      "remote: Enumerating objects: 548, done.\u001b[K\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 548 (delta 38), reused 42 (delta 20), pack-reused 455\u001b[K\n",
      "Receiving objects: 100% (548/548), 7.95 MiB | 13.38 MiB/s, done.\n",
      "Resolving deltas: 100% (313/313), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/Manan-dev/ChatPI.git\n",
    "\n",
    "!!pip3 install -r ./ChatPi/requirements.txt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Most of the functions are used for utility purposes throughout the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are our utility functions, used for various tasks throughout the project\n",
    "import os\n",
    "import glob\n",
    "import evaluate\n",
    "from termcolor import colored, cprint\n",
    "import numpy as np\n",
    "from termcolor import cprint, colored\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def read_context(fname, basepath=\"./content/sections\"):\n",
    "    globpath = os.path.join(basepath, f\"{fname}.*.md\")\n",
    "    print(\"GLOBPATH: \", globpath)\n",
    "    files = glob.glob(globpath)\n",
    "    print(\"FILES: \", files)\n",
    "    files = sorted(files)\n",
    "\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    print(f\"Found: {fnames}\")\n",
    "\n",
    "    for fname in files:\n",
    "        fnameparts = fname.split(\".\")\n",
    "        if not fnameparts[-2].isdigit():\n",
    "            # print(f\"Skipping: {fname}\")\n",
    "            continue\n",
    "        with open(fname, \"r\") as f:\n",
    "            text = f.read().strip()\n",
    "            yield fname, text\n",
    "\n",
    "\n",
    "def read_qa(fname, basepath=\"../sections\"):\n",
    "    fname = os.path.join(basepath, f\"{fname}.qa.md\")\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "        for qa_text in text.split(\"---\"):\n",
    "            qa = qa_text.strip().split(\"\\n\")\n",
    "            qa = [l for l in qa if l.strip()]\n",
    "\n",
    "            if len(qa) > 2:\n",
    "                raise ValueError(f\"Too many lines in QA:\\n{qa}\")\n",
    "\n",
    "            if len(qa) < 2:\n",
    "                raise ValueError(f\"Too few lines in QA:\\n{qa}\")\n",
    "\n",
    "            q = qa[0].strip()\n",
    "            a = qa[1].strip()\n",
    "\n",
    "            yield q, a\n",
    "\n",
    "\n",
    "def cscore(score: float):\n",
    "    if score > 0.9:\n",
    "        color = \"green\"\n",
    "    elif score > 0.8:\n",
    "        color = \"light_green\"\n",
    "    elif score > 0.65:\n",
    "        color = \"light_yellow\"\n",
    "    elif score > 0.5:\n",
    "        color = \"yellow\"\n",
    "    elif score > 0.25:\n",
    "        color = \"light_red\"\n",
    "    else:\n",
    "        color = \"red\"\n",
    "    return colored(f\"{round(score, 4)}\", color)\n",
    "\n",
    "\n",
    "def get_similarity_score(prediction: str, reference: str):\n",
    "    import spacy\n",
    "    from spacy.cli import download\n",
    "\n",
    "    file = \"en_core_web_lg\"\n",
    "\n",
    "    if not spacy.util.is_package(file):\n",
    "        download(file)\n",
    "    nlp = spacy.load(file)\n",
    "\n",
    "    # Process the sentences\n",
    "    doc1 = nlp(prediction)\n",
    "    doc2 = nlp(reference)\n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    doc1 = [t for t in doc1 if not t.is_stop and not t.is_punct]\n",
    "    doc2 = [t for t in doc2 if not t.is_stop and not t.is_punct]\n",
    "\n",
    "    # combine into a single doc\n",
    "    doc1 = nlp(\" \".join([t.text for t in doc1]))\n",
    "    doc2 = nlp(\" \".join([t.text for t in doc2]))\n",
    "\n",
    "    # Compute the similarity score\n",
    "    score = doc1.similarity(doc2)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_eval_score(\n",
    "    prediction: str,\n",
    "    reference: str,\n",
    "    metric: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    prediction = prediction.strip().lower()\n",
    "    reference = reference.strip().lower()\n",
    "\n",
    "    if metric == \"spacy_sim\":\n",
    "        score = get_similarity_score(prediction, reference)\n",
    "        return dict(spacy_sim=score)\n",
    "\n",
    "    m = evaluate.load(metric)\n",
    "\n",
    "    # dictionary containing the evaluation metric values\n",
    "    m_dict: dict = m.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[reference],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # iterate over the dictionary and prepend the metric name to the key\n",
    "    # if not already present (e.g. \"bertscore\" -> \"bertscore_f1\")\n",
    "    # m_dict = {f\"{metric}_{k}\": v for k, v in m_dict.items() if not k.startswith(metric)}\n",
    "    m_dict_new = {}\n",
    "    for k, v in m_dict.items():\n",
    "        if not k.startswith(metric):\n",
    "            k = f\"{metric}_{k}\"\n",
    "        m_dict_new[k] = v\n",
    "\n",
    "    # if any of the values are lists with just 1 element, then unpack the list\n",
    "    m_dict_new = {\n",
    "        k: v[0] if isinstance(v, list) and len(v) == 1 else v\n",
    "        for k, v in m_dict_new.items()\n",
    "    }\n",
    "\n",
    "    assert len(m_dict_new) > 0, f\"Metric: {metric} returned empty dict\"\n",
    "\n",
    "    return m_dict_new\n",
    "\n",
    "\n",
    "def create_plots(\n",
    "    ctx_name: str,\n",
    "    scores_by_model: dict[str, list],\n",
    "    scores_by_answer: dict[str, dict[str, list]],\n",
    "    scores_by_question: dict[str, dict[str, dict]],\n",
    "    tablefmt=\"double_grid\",\n",
    "    savedir=\"./plots\",\n",
    "):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"Plotting\")\n",
    "\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    models = list(scores_by_model.keys())\n",
    "    num_models = len(models)\n",
    "    num_questions = len(scores_by_question[models[0]])\n",
    "\n",
    "    print(f\"Models: {models}\")\n",
    "    print(f\"Questions: {num_questions}\")\n",
    "\n",
    "    #############################################################################\n",
    "    # scores_by_question\n",
    "\n",
    "    # Plot Version\n",
    "    # First subplots are individual models\n",
    "    fig = plt.figure(figsize=((num_models + 1) * 5, 5))\n",
    "    all_scores = []\n",
    "    for i, m in enumerate(models):\n",
    "        ax = fig.add_subplot(1, len(models) + 1, i + 1)\n",
    "        scores = [d[\"score\"] for d in scores_by_question[m]]\n",
    "        all_scores.append(scores)\n",
    "        ax.bar(range(len(scores)), scores)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        ax.set_title(m)\n",
    "        ax.set_ylabel(\"Evaluation Score\")\n",
    "        ax.set_xlabel(\"Question Index\")\n",
    "\n",
    "    # Last subplot is average across all models\n",
    "    ax = fig.add_subplot(1, len(models) + 1, len(models) + 1)\n",
    "    # convert to numpy array and average across axis 0\n",
    "    all_scores = np.array(all_scores)\n",
    "    avg_scores = np.mean(all_scores, axis=0)\n",
    "    ax.bar(range(len(avg_scores)), avg_scores)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_title(\"Average\")\n",
    "    ax.set_ylabel(\"Evaluation Score\")\n",
    "    ax.set_xlabel(\"Question Index\")\n",
    "\n",
    "    fig.suptitle(f\"QA Score by Question - (CTX: {ctx_name})\")\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(savedir, f\"{ctx_name}.scores_by_question.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Table Version\n",
    "    headers = [\"Q Idx\", \"Model\", \"Score\", \"Question\", \"Answer\", \"Expected Answer\"]\n",
    "    table = []\n",
    "    scores = []\n",
    "    for model, questions in scores_by_question.items():\n",
    "        for i, data in enumerate(questions):\n",
    "            score = data[\"score\"]\n",
    "            scores.append(score)\n",
    "            question = data[\"question\"]\n",
    "            answer = data[\"answer\"]\n",
    "            expected_answer = data[\"expected_answer\"]\n",
    "            table.append([i, model, cscore(score), question, answer, expected_answer])\n",
    "\n",
    "    # sort by question index\n",
    "    table = sorted(table, key=lambda x: x[0])\n",
    "\n",
    "    # last row for average across the scores\n",
    "    avg_score = np.mean(scores)\n",
    "    table.append([\"Avg\", \"-\", cscore(avg_score), \"-\", \"-\"])\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            table,\n",
    "            headers=headers,\n",
    "            tablefmt=tablefmt,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #############################################################################\n",
    "    # scores_by_answer\n",
    "    fig = plt.figure(figsize=(num_models * 5, 5))\n",
    "\n",
    "    # Boxplot Version\n",
    "    expected_answers = list(scores_by_answer[models[0]].keys())\n",
    "    for i, m in enumerate(models):\n",
    "        ax = fig.add_subplot(1, len(models), i + 1)\n",
    "        ax.boxplot(scores_by_answer[m].values())\n",
    "        ax.set_xticklabels(range(len(expected_answers)))\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        ax.set_title(m)\n",
    "        ax.set_ylabel(\"Evaluation Score\")\n",
    "        ax.set_xlabel(\"Expected Answer Group\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"QA Score by Expected Answer - {num_questions} Q's each (CTX: {ctx_name})\"\n",
    "    )\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(savedir, f\"{ctx_name}.scores_by_answer.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Table Version\n",
    "    headers = [\"Model\", \"A Idx\", \"Expected Answer\", \"Min\", \"Mean\", \"Max\"]\n",
    "    table = []\n",
    "    min_scores, mean_scores, max_scores = [], [], []\n",
    "    for model, answers in scores_by_answer.items():\n",
    "        for i, a in enumerate(answers.keys()):\n",
    "            scores = answers[a]\n",
    "            smin = min(scores)\n",
    "            smean = sum(scores) / len(scores)\n",
    "            smax = max(scores)\n",
    "            min_scores.append(smin)\n",
    "            mean_scores.append(smean)\n",
    "            max_scores.append(smax)\n",
    "            table.append(\n",
    "                [\n",
    "                    model,\n",
    "                    i,\n",
    "                    a,\n",
    "                    cscore(smin),\n",
    "                    cscore(smean),\n",
    "                    cscore(smax),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # sort by answer index\n",
    "    table = sorted(table, key=lambda x: x[1])\n",
    "\n",
    "    # average across the scores (min, mean, max)\n",
    "    min_avg, mean_avg, max_avg = (\n",
    "        np.mean(min_scores),\n",
    "        np.mean(mean_scores),\n",
    "        np.mean(max_scores),\n",
    "    )\n",
    "    table.append(\n",
    "        [\n",
    "            \"Avg\",\n",
    "            \"-\",\n",
    "            \"-\",\n",
    "            cscore(min_avg),\n",
    "            cscore(mean_avg),\n",
    "            cscore(max_avg),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            table,\n",
    "            headers=headers,\n",
    "            tablefmt=tablefmt,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #############################################################################\n",
    "    # scores_by_model\n",
    "\n",
    "    # Boxplot Version\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot(scores_by_model.values())\n",
    "    ax.set_xticklabels(scores_by_model.keys(), rotation=10, ha=\"right\")\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_title(f\"QA Score by Model - {num_questions} Q's each (CTX: {ctx_name})\")\n",
    "    ax.set_ylabel(\"Evaluation Score\")\n",
    "    ax.set_xlabel(\"Model ID\")\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(savedir, f\"{ctx_name}.scores_by_model.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Table Version\n",
    "    headers = [\"Model\", \"Min\", \"Mean\", \"Max\"]\n",
    "    table = []\n",
    "    min_scores, mean_scores, max_scores = [], [], []\n",
    "    for i, m in enumerate(scores_by_model.keys()):\n",
    "        scores = scores_by_model[m]\n",
    "        smin = min(scores)\n",
    "        smean = sum(scores) / len(scores)\n",
    "        smax = max(scores)\n",
    "        min_scores.append(smin)\n",
    "        mean_scores.append(smean)\n",
    "        max_scores.append(smax)\n",
    "        table.append(\n",
    "            [\n",
    "                m,\n",
    "                cscore(smin),\n",
    "                cscore(smean),\n",
    "                cscore(smax),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # average across the scores (min, mean, max)\n",
    "    min_avg, mean_avg, max_avg = (\n",
    "        np.mean(min_scores),\n",
    "        np.mean(mean_scores),\n",
    "        np.mean(max_scores),\n",
    "    )\n",
    "    table.append([\"Avg\", cscore(min_avg), cscore(mean_avg), cscore(max_avg)])\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            table,\n",
    "            headers=headers,\n",
    "            tablefmt=tablefmt,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Question-Answering pipeline\n",
    "\n",
    "**Objective** - Implement a prompt interface that takes in a question, runs it through the question-answering pipeline and returns the answer\n",
    "\n",
    "Tasks:\n",
    "- Find five 300-words sections from a book that introduces the following:\n",
    "  - Protagonist\n",
    "  - Antagonist\n",
    "  - Crime and crime scene\n",
    "  - Any significant evidence\n",
    "  - Resolution of crime/a narrative that presents the case against perpetrator\n",
    "\n",
    "- Ask the model questions and return the answers\n",
    "- Document the results\n",
    "- Use different Question-Answering model to do same tasks mentioned above and document differences in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from .utils import get_similarity_score, get_eval_score\n",
    "\n",
    "\n",
    "def run_qa(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    model: Optional[str] = None,\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"-\" * 80)\n",
    "    match verbosity:\n",
    "        case 1:\n",
    "            print(f\"model: {model}\")\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        # model=\"deepset/roberta-base-squad2\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    question = question.strip()\n",
    "    context = context.strip()\n",
    "\n",
    "    match verbosity:\n",
    "        case 0:\n",
    "            pass\n",
    "        case 1:\n",
    "            print(f\"Q: {question}\")\n",
    "        case 2:\n",
    "            print(f\"C: {context}\")\n",
    "            print(f\"Q: {question}\")\n",
    "\n",
    "    # display(Markdown(f\"**Q:** {question}\"))\n",
    "    # display(Markdown(f\"**C:** {context}\"))\n",
    "\n",
    "    res = pipe(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    answer, score = \"idk\", 1.0\n",
    "\n",
    "    # Get the result\n",
    "    if res and isinstance(res, dict):\n",
    "        answer = res.get(\"answer\", \"idk\")\n",
    "        score = res.get(\"score\", 1.0)\n",
    "\n",
    "    answer = answer.strip()\n",
    "    score = round(score, 3)\n",
    "\n",
    "    print(f\"A: {answer} (model confidence score: {round(score, 3)})\")\n",
    "    # display(Markdown(f\"**A:** {answer} (score: {score})\"))\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def run_qa_models(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    models: list[str],\n",
    "    answer_true: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    if answer_true is None:\n",
    "        answer_true = \"\"\n",
    "\n",
    "    answer_preds = []\n",
    "    score_dicts = []\n",
    "\n",
    "    for model in models:\n",
    "        answer_pred = run_qa(\n",
    "            question,\n",
    "            context,\n",
    "            model=model,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # compare the predicted answer to the true answer\n",
    "        score_dict_1 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"spacy_sim\",\n",
    "        )\n",
    "        score_dict_2 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"bertscore\",\n",
    "            lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        )\n",
    "        score_dict_3 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"rouge\",\n",
    "        )\n",
    "\n",
    "        score_dict = {**score_dict_1, **score_dict_2, **score_dict_3}\n",
    "        pprint(score_dict)\n",
    "\n",
    "        answer_preds.append(answer_pred)\n",
    "        score_dicts.append(score_dict)\n",
    "\n",
    "    return answer_preds, score_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Translation pipeline (French)\n",
    "\n",
    "**Objective** - Utilize a translation pipeline that translates the answers found in Part 1 into French and back to English Tasks:\n",
    "\n",
    "```\n",
    "> Question\n",
    "> Answer in English\n",
    "> Answer in French\n",
    "> Answer in English, translated from French\n",
    "```\n",
    "\n",
    "- Document the results\n",
    "- Use different Translation model to do same tasks mentioned above and document differences in the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def run_tr(\n",
    "    text: str,\n",
    "    model: Optional[str] = None,\n",
    "    pipeline_name: str = \"translation_en_to_fr\",\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"-\" * 100)\n",
    "    match verbosity:\n",
    "        case 1:\n",
    "            print(f\"model: {model}\")\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        pipeline_name,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # match verbosity:\n",
    "    #     case 0:\n",
    "    #         pass\n",
    "    #     case 1:\n",
    "    #         print(f\"> {text}\")\n",
    "\n",
    "    res = pipe(\n",
    "        text,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    # Get the result\n",
    "    translation_text = \"idk\"\n",
    "    if res and isinstance(res, list):\n",
    "        assert len(res) == 1, \"Expected only 1 result\"\n",
    "        translation_text = res[0].get(\"translation_text\", \"idk\")\n",
    "\n",
    "    translation_text = translation_text.strip()\n",
    "\n",
    "    # print()\n",
    "    print(f\"> {translation_text}\")\n",
    "    return translation_text\n",
    "\n",
    "\n",
    "def run_tr_models(\n",
    "    text: str,\n",
    "    models: list[tuple[str]],\n",
    "    **kwargs,\n",
    "):\n",
    "    translation_preds = []\n",
    "    score_dicts = []\n",
    "\n",
    "    for model_en_to_fr, model_fr_to_en in models:\n",
    "        # translate original text from english to french\n",
    "        text_fr = run_tr(\n",
    "            text, model=model_en_to_fr, pipeline_name=\"translation_en_to_fr\", **kwargs\n",
    "        )\n",
    "        # translate the french text back to english\n",
    "        text_en = run_tr(\n",
    "            text_fr,\n",
    "            model=model_fr_to_en,\n",
    "            pipeline_name=\"translation_fr_to_en\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # evaluate by comparing the text translated back to english to the original english text\n",
    "        score_dict_1 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"spacy_sim\",\n",
    "        )\n",
    "        score_dict_2 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"bertscore\",\n",
    "            lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        )\n",
    "        score_dict_3 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"rouge\",\n",
    "        )\n",
    "\n",
    "        score_dict = {**score_dict_1, **score_dict_2, **score_dict_3}\n",
    "        pprint(score_dict)\n",
    "\n",
    "        translation_preds.append((text_fr, text_en))\n",
    "        score_dicts.append(score_dict)\n",
    "\n",
    "    return translation_preds, score_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Summarization pipeline\n",
    "\n",
    "**Objective** - Utilize a text summarization pipeline that summarizes the 300-words sections found in Part 1 Tasks:\n",
    "\n",
    "- Run the five 300-words sections through pipeline\n",
    "- Document the results\n",
    "- Use different text summarization model to do same tasks mentioned above and document differences in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "def run_sum(\n",
    "    text: str,\n",
    "    model: Optional[str] = None,\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    # TODO: set some good defaults here?\n",
    "    # kwargs.setdefault(\"min_length\", 5)\n",
    "    # kwargs.setdefault(\"max_length\", 20)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    match verbosity:\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"summarization\",\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    match verbosity:\n",
    "        case 1 | 2:\n",
    "            print(f\"> {text}\")\n",
    "\n",
    "    res = pipe(\n",
    "        text,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    # Get the result\n",
    "    summary_text = \"idk\"\n",
    "    if res and isinstance(res, list):\n",
    "        assert len(res) == 1, \"Expected only 1 result\"\n",
    "        summary_text = res[0].get(\"summary_text\", \"idk\")\n",
    "\n",
    "    summary_text = summary_text.strip()\n",
    "\n",
    "    # print()\n",
    "    print(f\"> {summary_text}\")\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "\n",
    "def run_sum_models(\n",
    "    text: str,\n",
    "    models: list[str],\n",
    "    expected_answer: str,\n",
    "    metric: str = \"spacy_sim\",\n",
    "    **kwargs,\n",
    "):\n",
    "    if expected_answer is None:\n",
    "        expected_answer = \"\"\n",
    "\n",
    "    answers = []\n",
    "    scores = []\n",
    "\n",
    "    for model in models:\n",
    "        a = run(text, model)\n",
    "        s = get_eval_score(a, expected_answer, metric, **kwargs)\n",
    "\n",
    "        answers.append(a)\n",
    "        scores.append(s)\n",
    "\n",
    "    return answers, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ChatBot Implementation\n",
    "\n",
    "Implement the ChatBot functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "]\n",
    "\n",
    "def QA_Func(ctx_name, question):\n",
    "  for ctx_idx, (ctx_fname, ctx_text) in enumerate(read_context(ctx_name)):\n",
    "    ctx_fname = os.path.basename(ctx_fname)\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx_text)\n",
    "    \n",
    "\n",
    "    # for q_idx, (q_text, q_answer_true) in enumerate(read_qa(ctx_name)):\n",
    "    #   print(\"=\" * 80)\n",
    "    #   print(\"=\" * 80)\n",
    "    #   print(f\"Current Question: {q_text}\")\n",
    "    #   print(f\"Expected Answer: {q_answer_true}\")\n",
    "\n",
    "    q_answers_preds, q_answers_scores = run_qa_models(\n",
    "      q_text,\n",
    "      ctx_text,\n",
    "      model,\n",
    "      q_answer_true,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call for qa\n",
    "# function to call for translation\n",
    "# function to call for summarization\n",
    "import sys\n",
    "\n",
    "logo = \"\"\"\n",
    "=================================\n",
    "  ____ _           _   ____ ___ \n",
    " / ___| |__   __ _| |_|  _ \\_ _|\n",
    "| |   | '_ \\ / _` | __| |_) | | \n",
    "| |___| | | | (_| | |_|  __/| | \n",
    " \\____|_| |_|\\__,_|\\__|_|  |___|\n",
    " \n",
    "=================================\n",
    "    \"\"\"\n",
    "print(logo)\n",
    "print(\"Welcome! Please select a context!\")\n",
    "print(\"[1] Protagonist\\n[2] Antagonist\\n[3] Crime\\n[4] Evidence\\n[5] Resolution\\n\")\n",
    "print(\"Type 'help' for more commands!\")\n",
    "\n",
    "context_dictionary = {\n",
    "    1: \"protagonist\",\n",
    "    2: \"antagonist\",\n",
    "    3: \"crime\",\n",
    "    4: \"evidence\",\n",
    "    5: \"resolution\"\n",
    "}\n",
    "\n",
    "# Pick context\n",
    "# Give example questions to use for said context\n",
    "# When asked, run the pipeline and return \n",
    "\n",
    "while True:  \n",
    "    user_input = input(\"> \")\n",
    "\n",
    "    match user_input.lower():\n",
    "      case 'protagonist' | '1':\n",
    "        print(\" Loading protagonist\\n\")\n",
    "        # handle qa and translation stuff first\n",
    "        ctx_name = 'protagonist'\n",
    "        QA_Func(ctx_name, \"Who is the main character that the story revolves around?\")\n",
    "\n",
    "      case 'antagonist' | '2':\n",
    "        print(\" Loading antagonist\\n\")\n",
    "\n",
    "      case 'crime' | '3':\n",
    "        print(\" Loading crime\\n\")\n",
    "\n",
    "      case 'evidence' | '4':\n",
    "        print(\" Loading evidence\\n\")\n",
    "\n",
    "      case 'resolution' | '5':\n",
    "        print(\" Loading resolution\\n\")\n",
    "\n",
    "      case 'help':\n",
    "        print(\"Select a context: \\n[1] Protagonist\\n[2] Antagonist\\n[3] Crime\\n[4] Evidence\\n[5] Resolution\\n\\nOr type\\nHelp -- list commands\\nQuit -- exit the program\\n\")\n",
    "\n",
    "      case 'quit':\n",
    "        print(\"exiting program...\\n\")\n",
    "        sys.exit()\n",
    "\n",
    "      case _:\n",
    "        print(\"please enter a command\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
