{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Manan-dev/ChatPI.git@final-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Most of the functions are used for utility purposes throughout the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are our utility functions, used for various tasks throughout the project\n",
    "import os\n",
    "import glob\n",
    "import evaluate\n",
    "from termcolor import colored, cprint\n",
    "import numpy as np\n",
    "from termcolor import cprint, colored\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def read_context(fname, basepath=\"./content/sections\"):\n",
    "    globpath = os.path.join(basepath, f\"{fname}.*.md\")\n",
    "    print(\"GLOBPATH: \", globpath)\n",
    "    files = glob.glob(globpath)\n",
    "    print(\"FILES: \", files)\n",
    "    files = sorted(files)\n",
    "\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    print(f\"Found: {fnames}\")\n",
    "\n",
    "    for fname in files:\n",
    "        fnameparts = fname.split(\".\")\n",
    "        if not fnameparts[-2].isdigit():\n",
    "            # print(f\"Skipping: {fname}\")\n",
    "            continue\n",
    "        with open(fname, \"r\") as f:\n",
    "            text = f.read().strip()\n",
    "            yield fname, text\n",
    "\n",
    "\n",
    "def read_quiz(fname, basepath=\"../sections\"):\n",
    "    fname = os.path.join(basepath, f\"{fname}.qa.md\")\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "        for qa_text in text.split(\"---\"):\n",
    "            qa = qa_text.strip().split(\"\\n\")\n",
    "            qa = [l for l in qa if l.strip()]\n",
    "\n",
    "            if len(qa) > 2:\n",
    "                raise ValueError(f\"Too many lines in QA:\\n{qa}\")\n",
    "\n",
    "            if len(qa) < 2:\n",
    "                raise ValueError(f\"Too few lines in QA:\\n{qa}\")\n",
    "\n",
    "            q = qa[0].strip()\n",
    "            a = qa[1].strip()\n",
    "\n",
    "            yield q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Question-Answering pipeline\n",
    "\n",
    "**Objective** - Implement a prompt interface that takes in a question, runs it through the question-answering pipeline and returns the answer\n",
    "\n",
    "Tasks:\n",
    "- Find five 300-words sections from a book that introduces the following:\n",
    "  - Protagonist\n",
    "  - Antagonist\n",
    "  - Crime and crime scene\n",
    "  - Any significant evidence\n",
    "  - Resolution of crime/a narrative that presents the case against perpetrator\n",
    "\n",
    "- Ask the model questions and return the answers\n",
    "- Document the results\n",
    "- Use different Question-Answering model to do same tasks mentioned above and document differences in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "# from .utils import get_similarity_score, get_eval_score\n",
    "\n",
    "\n",
    "def run_qa(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    model: Optional[str] = None,\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"-\" * 80)\n",
    "    match verbosity:\n",
    "        case 1:\n",
    "            print(f\"model: {model}\")\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        # model=\"deepset/roberta-base-squad2\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    question = question.strip()\n",
    "    context = context.strip()\n",
    "\n",
    "    match verbosity:\n",
    "        case 0:\n",
    "            pass\n",
    "        case 1:\n",
    "            print(f\"Q: {question}\")\n",
    "        case 2:\n",
    "            print(f\"C: {context}\")\n",
    "            print(f\"Q: {question}\")\n",
    "\n",
    "    # display(Markdown(f\"**Q:** {question}\"))\n",
    "    # display(Markdown(f\"**C:** {context}\"))\n",
    "\n",
    "    res = pipe(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    answer, score = \"idk\", 1.0\n",
    "\n",
    "    # Get the result\n",
    "    if res and isinstance(res, dict):\n",
    "        answer = res.get(\"answer\", \"idk\")\n",
    "        score = res.get(\"score\", 1.0)\n",
    "\n",
    "    answer = answer.strip()\n",
    "    score = round(score, 3)\n",
    "\n",
    "    print(f\"A: {answer} (model confidence score: {round(score, 3)})\")\n",
    "    # display(Markdown(f\"**A:** {answer} (score: {score})\"))\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def run_qa_models(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    models: list[str],\n",
    "    answer_true: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    if answer_true is None:\n",
    "        answer_true = \"\"\n",
    "\n",
    "    answer_preds = []\n",
    "    score_dicts = []\n",
    "\n",
    "    for model in models:\n",
    "        answer_pred = run_qa(\n",
    "            question,\n",
    "            context,\n",
    "            model=model,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # compare the predicted answer to the true answer\n",
    "        score_dict_1 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"spacy_sim\",\n",
    "        )\n",
    "        score_dict_2 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"bertscore\",\n",
    "            lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        )\n",
    "        score_dict_3 = get_eval_score(\n",
    "            answer_pred,\n",
    "            answer_true,\n",
    "            metric=\"rouge\",\n",
    "        )\n",
    "\n",
    "        score_dict = {**score_dict_1, **score_dict_2, **score_dict_3}\n",
    "        pprint(score_dict)\n",
    "\n",
    "        answer_preds.append(answer_pred)\n",
    "        score_dicts.append(score_dict)\n",
    "\n",
    "    return answer_preds, score_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Translation pipeline (French)\n",
    "\n",
    "**Objective** - Utilize a translation pipeline that translates the answers found in Part 1 into French and back to English Tasks:\n",
    "\n",
    "```\n",
    "> Question\n",
    "> Answer in English\n",
    "> Answer in French\n",
    "> Answer in English, translated from French\n",
    "```\n",
    "\n",
    "- Document the results\n",
    "- Use different Translation model to do same tasks mentioned above and document differences in the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def run_tr(\n",
    "    text: str,\n",
    "    model: Optional[str] = None,\n",
    "    pipeline_name: str = \"translation_en_to_fr\",\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"-\" * 100)\n",
    "    match verbosity:\n",
    "        case 1:\n",
    "            print(f\"model: {model}\")\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        pipeline_name,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # match verbosity:\n",
    "    #     case 0:\n",
    "    #         pass\n",
    "    #     case 1:\n",
    "    #         print(f\"> {text}\")\n",
    "\n",
    "    res = pipe(\n",
    "        text,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    # Get the result\n",
    "    translation_text = \"idk\"\n",
    "    if res and isinstance(res, list):\n",
    "        assert len(res) == 1, \"Expected only 1 result\"\n",
    "        translation_text = res[0].get(\"translation_text\", \"idk\")\n",
    "\n",
    "    translation_text = translation_text.strip()\n",
    "\n",
    "    # print()\n",
    "    print(f\"> {translation_text}\")\n",
    "    return translation_text\n",
    "\n",
    "\n",
    "def run_tr_models(\n",
    "    text: str,\n",
    "    models: list[tuple[str]],\n",
    "    **kwargs,\n",
    "):\n",
    "    translation_preds = []\n",
    "    score_dicts = []\n",
    "\n",
    "    for model_en_to_fr, model_fr_to_en in models:\n",
    "        # translate original text from english to french\n",
    "        text_fr = run_tr(\n",
    "            text, model=model_en_to_fr, pipeline_name=\"translation_en_to_fr\", **kwargs\n",
    "        )\n",
    "        # translate the french text back to english\n",
    "        text_en = run_tr(\n",
    "            text_fr,\n",
    "            model=model_fr_to_en,\n",
    "            pipeline_name=\"translation_fr_to_en\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # evaluate by comparing the text translated back to english to the original english text\n",
    "        score_dict_1 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"spacy_sim\",\n",
    "        )\n",
    "        score_dict_2 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"bertscore\",\n",
    "            lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        )\n",
    "        score_dict_3 = get_eval_score(\n",
    "            text_en,\n",
    "            text,\n",
    "            metric=\"rouge\",\n",
    "        )\n",
    "\n",
    "        score_dict = {**score_dict_1, **score_dict_2, **score_dict_3}\n",
    "        pprint(score_dict)\n",
    "\n",
    "        translation_preds.append((text_fr, text_en))\n",
    "        score_dicts.append(score_dict)\n",
    "\n",
    "    return translation_preds, score_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Summarization pipeline\n",
    "\n",
    "**Objective** - Utilize a text summarization pipeline that summarizes the 300-words sections found in Part 1 Tasks:\n",
    "\n",
    "- Run the five 300-words sections through pipeline\n",
    "- Document the results\n",
    "- Use different text summarization model to do same tasks mentioned above and document differences in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "def run_sum(\n",
    "    text: str,\n",
    "    model: Optional[str] = None,\n",
    "    verbosity: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    # TODO: set some good defaults here?\n",
    "    # kwargs.setdefault(\"min_length\", 5)\n",
    "    # kwargs.setdefault(\"max_length\", 20)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    match verbosity:\n",
    "        case 2:\n",
    "            print(f\"model: {model}\")\n",
    "            for k, v in kwargs.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "            print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"summarization\",\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    match verbosity:\n",
    "        case 1 | 2:\n",
    "            print(f\"> {text}\")\n",
    "\n",
    "    res = pipe(\n",
    "        text,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    # Get the result\n",
    "    summary_text = \"idk\"\n",
    "    if res and isinstance(res, list):\n",
    "        assert len(res) == 1, \"Expected only 1 result\"\n",
    "        summary_text = res[0].get(\"summary_text\", \"idk\")\n",
    "\n",
    "    summary_text = summary_text.strip()\n",
    "\n",
    "    # print()\n",
    "    print(f\"> {summary_text}\")\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "\n",
    "def run_sum_models(\n",
    "    text: str,\n",
    "    models: list[str],\n",
    "    expected_answer: str,\n",
    "    metric: str = \"spacy_sim\",\n",
    "    **kwargs,\n",
    "):\n",
    "    if expected_answer is None:\n",
    "        expected_answer = \"\"\n",
    "\n",
    "    answers = []\n",
    "    scores = []\n",
    "\n",
    "    for model in models:\n",
    "        a = run(text, model)\n",
    "        s = get_eval_score(a, expected_answer, metric, **kwargs)\n",
    "\n",
    "        answers.append(a)\n",
    "        scores.append(s)\n",
    "\n",
    "    return answers, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ChatBot Implementation\n",
    "\n",
    "Implement the ChatBot functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "]\n",
    "\n",
    "def QA_Func(ctx_name, question):\n",
    "  for ctx_idx, (ctx_fname, ctx_text) in enumerate(read_context(ctx_name)):\n",
    "    ctx_fname = os.path.basename(ctx_fname)\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call for qa\n",
    "# function to call for translation\n",
    "# function to call for summarization\n",
    "import sys\n",
    "\n",
    "logo = \"\"\"\n",
    "=================================\n",
    "  ____ _           _   ____ ___ \n",
    " / ___| |__   __ _| |_|  _ \\_ _|\n",
    "| |   | '_ \\ / _` | __| |_) | | \n",
    "| |___| | | | (_| | |_|  __/| | \n",
    " \\____|_| |_|\\__,_|\\__|_|  |___|\n",
    " \n",
    "=================================\n",
    "    \"\"\"\n",
    "print(logo)\n",
    "print(\"Welcome! Please select a context!\")\n",
    "print(\"[1] Protagonist\\n[2] Antagonist\\n[3] Crime\\n[4] Evidence\\n[5] Resolution\\n\")\n",
    "print(\"Type 'help' for more commands!\")\n",
    "\n",
    "context_dictionary = {\n",
    "    1: \"protagonist\",\n",
    "    2: \"antagonist\",\n",
    "    3: \"crime\",\n",
    "    4: \"evidence\",\n",
    "    5: \"resolution\"\n",
    "}\n",
    "\n",
    "# Pick context\n",
    "# Give example questions to use for said context\n",
    "# When asked, run the pipeline and return \n",
    "\n",
    "while True:  \n",
    "    user_input = input(\"> \")\n",
    "\n",
    "    match user_input.lower():\n",
    "      case 'protagonist' | '1':\n",
    "        print(\" Loading protagonist\\n\")\n",
    "        # handle qa and translation stuff first\n",
    "        ctx_name = 'protagonist'\n",
    "        QA_Func(ctx_name, \"Who is the main character that the story revolves around?\")\n",
    "\n",
    "      case 'antagonist' | '2':\n",
    "        print(\" Loading antagonist\\n\")\n",
    "\n",
    "      case 'crime' | '3':\n",
    "        print(\" Loading crime\\n\")\n",
    "\n",
    "      case 'evidence' | '4':\n",
    "        print(\" Loading evidence\\n\")\n",
    "\n",
    "      case 'resolution' | '5':\n",
    "        print(\" Loading resolution\\n\")\n",
    "\n",
    "      case 'help':\n",
    "        print(\"Select a context: \\n[1] Protagonist\\n[2] Antagonist\\n[3] Crime\\n[4] Evidence\\n[5] Resolution\\n\\nOr type\\nHelp -- list commands\\nQuit -- exit the program\\n\")\n",
    "\n",
    "      case 'quit':\n",
    "        print(\"exiting program...\\n\")\n",
    "        sys.exit()\n",
    "\n",
    "      case _:\n",
    "        print(\"please enter a command\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e22577b96ae4292ce4f56ac62ee5d572607fd90aee9aa8e67f8a1f5a6d69949d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
