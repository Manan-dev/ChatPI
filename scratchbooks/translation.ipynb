{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Translation\n",
    "\n",
    "You will modify Part 1 to generate the translations of your answers from Part 1 into a particular language (see below) and then back to English.\n",
    "\n",
    "So, your prompt should look like:\n",
    "\n",
    "> Your question.  \n",
    "> Answer in English.  \n",
    "> Answer in the assigned Language.  \n",
    "> Answer in English, translated from the above language.\n",
    "\n",
    "The language you will use for your project is:\n",
    "\n",
    "Team 1, 4, 7, 10, 13 - Spanish\n",
    "\n",
    "Team 2,5,8,11 - German\n",
    "\n",
    "Team 3,6,9,12 - French\n",
    "\n",
    "Observe the effects of the cyclical translation (e.g., English->French->English) and critique the results in your slides and the report.\n",
    "\n",
    "Part 2.2 -- use two different HF translation models: use the default translation pipeline, then use other models of choice and discuss the differences in the result.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.TranslationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    text: str,\n",
    "    model: Optional[str] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"model: {model}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"translation_en_to_fr\",\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    text = text.strip()\n",
    "    print(f\"> {text}\")\n",
    "\n",
    "    res = pipe(\n",
    "        text,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    # Get the result\n",
    "    translation_text = \"idk\"\n",
    "    if res and isinstance(res, list):\n",
    "        assert len(res) == 1, \"Expected only 1 result\"\n",
    "        translation_text = res[0].get(\"translation_text\", \"idk\")\n",
    "\n",
    "    translation_text = translation_text.strip()\n",
    "\n",
    "    # print()\n",
    "    print(f\"> {translation_text}\")\n",
    "    return translation_text\n",
    "\n",
    "\n",
    "def run_models(text: str, models: list[str], **kwargs):\n",
    "    for model in models:\n",
    "        run(text, model=model, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Prompt Interface\n",
    "\n",
    "Models: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = \"\"\"\n",
    "Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
    "his hypodermic syringe from its neat morocco case. With his long,\n",
    "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
    "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
    "upon the sinewy forearm and wrist all dotted and scarred with\n",
    "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
    "pressed down the tiny piston, and sank back into the velvet-lined\n",
    "arm-chair with a long sigh of satisfaction.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: None\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acozma/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes a pris sa bouteille du coin du manteau et sa seringue hypodermique de son étui morocco. Avec ses longs doigts blancs et nerveux, il ajusta la délicate aiguille et roulait à l'envers de sa manche gauche. Pendant un peu de temps, ses yeux reposèrent soigneusement sur l'avant-bras\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/t5-base\n",
    "_ = run(test_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: t5-large\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acozma/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes prenait sa bouteille de l'angle du manteau et sa seringue hypodermique de son étui morocain soigné. Avec ses longs doigts blancs et nerveux, il ajustait l'aiguille délicate et roulait son manteau gauche. Pendant un peu de temps, ses yeux restaient soigneusement sur l'avant-bra\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/t5-large\n",
    "_ = run(test_txt, model=\"t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: Helsinki-NLP/opus-mt-en-fr\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes a pris sa bouteille du coin de la cheminée et sa seringue hypodermique de son boîtier morocco soigné. Avec ses doigts longs, blancs et nerveux, il a ajusté l'aiguille délicate, et a roulé sa chemise-cuisse gauche. Pendant un peu de temps, ses yeux reposaient délicatement sur l'avant-bras et le poignet sinueux tous parsemés et marqués d'innombrables marques de ponction. Enfin, il a poussé le point aigu à la maison, a pressé le petit piston, et a coulé dans la chaise de bras doublée de velours avec un long soupir de satisfaction.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/Helsinki-NLP/opus-mt-en-fr\n",
    "_ = run(test_txt, model=\"Helsinki-NLP/opus-mt-en-fr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments & Results\n",
    "\n",
    "You will modify Part 1 to generate the translations of your answers from Part 1 into a particular language (see below) and then back to English.\n",
    "\n",
    "Observe the effects of the cyclical translation (e.g., English->French->English) and critique the results in your slides and the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out a good selection of models and keep some interesting ones\n",
    "models = [\n",
    "    \"t5-base\",\n",
    "    \"t5-large\",\n",
    "    \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: t5-base\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes a pris sa bouteille du coin du manteau et sa seringue hypodermique de son étui morocco. Avec ses longs doigts blancs et nerveux, il ajusta la délicate aiguille et roulait à l'envers de sa manche gauche. Pendant un peu de temps, ses yeux reposèrent soigneusement sur l'avant-bras\n",
      "====================================================================================================\n",
      "model: t5-large\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes prenait sa bouteille de l'angle du manteau et sa seringue hypodermique de son étui morocain soigné. Avec ses longs doigts blancs et nerveux, il ajustait l'aiguille délicate et roulait son manteau gauche. Pendant un peu de temps, ses yeux restaient soigneusement sur l'avant-bra\n",
      "====================================================================================================\n",
      "model: Helsinki-NLP/opus-mt-en-fr\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "> Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "> Sherlock Holmes a pris sa bouteille du coin de la cheminée et sa seringue hypodermique de son boîtier morocco soigné. Avec ses doigts longs, blancs et nerveux, il a ajusté l'aiguille délicate, et a roulé sa chemise-cuisse gauche. Pendant un peu de temps, ses yeux reposaient délicatement sur l'avant-bras et le poignet sinueux tous parsemés et marqués d'innombrables marques de ponction. Enfin, il a poussé le point aigu à la maison, a pressé le petit piston, et a coulé dans la chaise de bras doublée de velours avec un long soupir de satisfaction.\n"
     ]
    }
   ],
   "source": [
    "## TODO: add experiments and results\n",
    "\n",
    "run_models(test_txt, models=models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
