{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Summarization\n",
    "\n",
    "For Part 3, use the same 300-word long sections as in Part 1, but instead of answering the questions about these sections, ask the ChatPI to summarize them for you, using Hugging Face summarization pipeline.\n",
    "\n",
    "Part 3.2 -- use two different HF summarization models: use the default summarization pipeline, then use other models of choice and discuss the differences in the result.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.SummarizationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ../requirements.txt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from src import utils\n",
    "from src.summarization import run, run_models\n",
    "import evaluate\n",
    "# from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments & Results\n",
    "\n",
    "Use the same 300-word long sections as in Part 1, but instead of answering the questions about these sections, ask the ChatPI to summarize them for you, using Hugging Face summarization pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out a good selection of models and keep some interesting ones\n",
    "models = [\n",
    "    \"sshleifer/distilbart-cnn-12-6\",\n",
    "    \"slauw87/bart_summarisation\",\n",
    "    \"pszemraj/pegasus-x-large-book-summary\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rouge1: unigram (1-gram) based scoring\n",
    "Rouge2: bigram (2-gram) based scoring\n",
    "RougeL: ROUGE-L measures the longest common subsequence between the generated summary and the reference summary.\n",
    "RougeLsum: splits text using \\n\n",
    "\"\"\"\n",
    "\n",
    "ctx = \"../sections/resolution.0.md\"\n",
    "text = \"\"\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "with open(ctx, \"r\") as f:\n",
    "    text = f.read().strip()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    sum_text = run(text, model=model)\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = rouge.compute(\n",
    "        predictions=[sum_text],\n",
    "        references=[text],\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    print(\"EVAL RESULTS FOR MODEL: \", model)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e22577b96ae4292ce4f56ac62ee5d572607fd90aee9aa8e67f8a1f5a6d69949d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
