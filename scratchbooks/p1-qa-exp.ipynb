{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Question Answering\n",
    "\n",
    "For the first part, use the Hugging Face question-answering pipeline and feed it with the five 300-word long sections from the book of your choice that you analyzed in Project 1.\n",
    "\n",
    "These sections should be selected so they are: introducing the protagonist(s), the antagonist, the crime and crime scene, any significant evidence, and the resolution of the crime/a narrative that presents the case against the perpetrator.\n",
    "\n",
    "For a prompt, Implement a simple prompt interface that takes in your question, runs it against the model, and returns the answer. You don't need to do anything special about this, just a simple console I/O interface without any complicated error handling. It is up to you how you want to upload the context to the model (pre-loaded into your program, on-demand, etc.).\n",
    "\n",
    "The questions you should ask are about the identity and characteristics of the protagonist, antagonist/perpetrator, the nature and the setting of the crime or crime scene, the evidence, and the case against the perpetrator.\n",
    "\n",
    "Document the questions, ask the questions, and document the specificity and accuracy of the results.\n",
    "\n",
    "Part 1.2 - use two different HF QA models: use the default question-answering pipeline, then use other models of choice and discuss the differences in the result.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ../requirements.txt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from src import utils\n",
    "from src.question_answering import run, run_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments & Results\n",
    "\n",
    "For the first part, use the Hugging Face question-answering pipeline and feed it with the five 300-word long sections from the book of your choice that you analyzed in Project 1.\n",
    "\n",
    "These sections should be selected so they are: **introducing the protagonist(s), the antagonist, the crime and crime scene, any significant evidence, and the resolution of the crime/a narrative that presents the case against the perpetrator.**\n",
    "\n",
    "The questions you should ask are about the identity and characteristics of the protagonist, antagonist/perpetrator, the nature and the setting of the crime or crime scene, the evidence, and the case against the perpetrator.\n",
    "\n",
    "Document the questions, ask the questions, and document the specificity and accuracy of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out a good selection of models and keep some interesting ones\n",
    "models = [\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_name = \"protagonist\"\n",
    "\n",
    "for i, (fname, ctx) in enumerate(utils.read_context(ctx_name)):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx)\n",
    "\n",
    "    # scores_by_question = {m: defaultdict(list) for m in models}\n",
    "    scores_by_question = defaultdict(list)\n",
    "    scores_by_answer = {m: defaultdict(list) for m in models}\n",
    "    scores_by_model = defaultdict(list)\n",
    "\n",
    "    for j, (question, true_answer) in enumerate(utils.read_qa(ctx_name)):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Current Question: {question}\")\n",
    "        print(f\"Expected Answer: {true_answer}\")\n",
    "\n",
    "        # for k in range(5):\n",
    "        answers, scores = run_models(\n",
    "            question,\n",
    "            ctx,\n",
    "            models,\n",
    "            expected_answer=true_answer,\n",
    "        )\n",
    "\n",
    "        for k, (m, s) in enumerate(zip(models, scores)):\n",
    "            scores_by_model[m].append(s)\n",
    "            scores_by_answer[m][true_answer].append(s)\n",
    "            scores_by_question[m].append(\n",
    "                dict(\n",
    "                    question=question,\n",
    "                    answer=answers[k],\n",
    "                    expected_answer=true_answer,\n",
    "                    score=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    utils.create_plots(ctx_name, scores_by_model, scores_by_answer, scores_by_question)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_name = \"antagonist\"\n",
    "\n",
    "for i, (fname, ctx) in enumerate(utils.read_context(ctx_name)):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx)\n",
    "\n",
    "    # scores_by_question = {m: defaultdict(list) for m in models}\n",
    "    scores_by_question = defaultdict(list)\n",
    "    scores_by_answer = {m: defaultdict(list) for m in models}\n",
    "    scores_by_model = defaultdict(list)\n",
    "\n",
    "    for j, (question, true_answer) in enumerate(utils.read_qa(ctx_name)):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Current Question: {question}\")\n",
    "        print(f\"Expected Answer: {true_answer}\")\n",
    "\n",
    "        # for k in range(5):\n",
    "        answers, scores = run_models(\n",
    "            question,\n",
    "            ctx,\n",
    "            models,\n",
    "            expected_answer=true_answer,\n",
    "        )\n",
    "\n",
    "        for k, (m, s) in enumerate(zip(models, scores)):\n",
    "            scores_by_model[m].append(s)\n",
    "            scores_by_answer[m][true_answer].append(s)\n",
    "            scores_by_question[m].append(\n",
    "                dict(\n",
    "                    question=question,\n",
    "                    answer=answers[k],\n",
    "                    score=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    utils.create_plots(ctx_name, scores_by_model, scores_by_answer, scores_by_question)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_name = \"crime\"\n",
    "\n",
    "for i, (fname, ctx) in enumerate(utils.read_context(ctx_name)):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx)\n",
    "\n",
    "    # scores_by_question = {m: defaultdict(list) for m in models}\n",
    "    scores_by_question = defaultdict(list)\n",
    "    scores_by_answer = {m: defaultdict(list) for m in models}\n",
    "    scores_by_model = defaultdict(list)\n",
    "\n",
    "    for j, (question, true_answer) in enumerate(utils.read_qa(ctx_name)):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Current Question: {question}\")\n",
    "        print(f\"Expected Answer: {true_answer}\")\n",
    "\n",
    "        # for k in range(5):\n",
    "        answers, scores = run_models(\n",
    "            question,\n",
    "            ctx,\n",
    "            models,\n",
    "            expected_answer=true_answer,\n",
    "        )\n",
    "\n",
    "        for k, (m, s) in enumerate(zip(models, scores)):\n",
    "            scores_by_model[m].append(s)\n",
    "            scores_by_answer[m][true_answer].append(s)\n",
    "            scores_by_question[m].append(\n",
    "                dict(\n",
    "                    question=question,\n",
    "                    answer=answers[k],\n",
    "                    score=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    utils.create_plots(ctx_name, scores_by_model, scores_by_answer, scores_by_question)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_name = \"evidence\"\n",
    "\n",
    "for i, (fname, ctx) in enumerate(utils.read_context(ctx_name)):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx)\n",
    "\n",
    "    # scores_by_question = {m: defaultdict(list) for m in models}\n",
    "    scores_by_question = defaultdict(list)\n",
    "    scores_by_answer = {m: defaultdict(list) for m in models}\n",
    "    scores_by_model = defaultdict(list)\n",
    "\n",
    "    for j, (question, true_answer) in enumerate(utils.read_qa(ctx_name)):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Current Question: {question}\")\n",
    "        print(f\"Expected Answer: {true_answer}\")\n",
    "\n",
    "        answers, scores = run_models(\n",
    "            question,\n",
    "            ctx,\n",
    "            models,\n",
    "            expected_answer=true_answer,\n",
    "        )\n",
    "\n",
    "        for k, (m, s) in enumerate(zip(models, scores)):\n",
    "            scores_by_model[m].append(s)\n",
    "            scores_by_answer[m][true_answer].append(s)\n",
    "            scores_by_question[m].append(\n",
    "                dict(\n",
    "                    question=question,\n",
    "                    answer=answers[k],\n",
    "                    score=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    utils.create_plots(ctx_name, scores_by_model, scores_by_answer, scores_by_question)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_name = \"resolution\"\n",
    "\n",
    "for i, (fname, ctx) in enumerate(utils.read_context(ctx_name)):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(ctx)\n",
    "\n",
    "    # scores_by_question = {m: defaultdict(list) for m in models}\n",
    "    scores_by_question = defaultdict(list)\n",
    "    scores_by_answer = {m: defaultdict(list) for m in models}\n",
    "    scores_by_model = defaultdict(list)\n",
    "\n",
    "    for j, (question, true_answer) in enumerate(utils.read_qa(ctx_name)):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Current Question: {question}\")\n",
    "        print(f\"Expected Answer: {true_answer}\")\n",
    "\n",
    "        # for k in range(5):\n",
    "        answers, scores = run_models(\n",
    "            question,\n",
    "            ctx,\n",
    "            models,\n",
    "            expected_answer=true_answer,\n",
    "        )\n",
    "\n",
    "        for k, (m, s) in enumerate(zip(models, scores)):\n",
    "            scores_by_model[m].append(s)\n",
    "            scores_by_answer[m][true_answer].append(s)\n",
    "            scores_by_question[m].append(\n",
    "                dict(\n",
    "                    question=question,\n",
    "                    answer=answers[k],\n",
    "                    score=s,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    utils.create_plots(ctx_name, scores_by_model, scores_by_answer, scores_by_question)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
