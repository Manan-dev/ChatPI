{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Question Answering\n",
    "\n",
    "For the first part, use the Hugging Face question-answering pipeline and feed it with the five 300-word long sections from the book of your choice that you analyzed in Project 1.\n",
    "\n",
    "These sections should be selected so they are: introducing the protagonist(s), the antagonist, the crime and crime scene, any significant evidence, and the resolution of the crime/a narrative that presents the case against the perpetrator.\n",
    "\n",
    "For a prompt, Implement a simple prompt interface that takes in your question, runs it against the model, and returns the answer. You don't need to do anything special about this, just a simple console I/O interface without any complicated error handling. It is up to you how you want to upload the context to the model (pre-loaded into your program, on-demand, etc.).\n",
    "\n",
    "The questions you should ask are about the identity and characteristics of the protagonist, antagonist/perpetrator, the nature and the setting of the crime or crime scene, the evidence, and the case against the perpetrator.\n",
    "\n",
    "Document the questions, ask the questions, and document the specificity and accuracy of the results.\n",
    "\n",
    "Part 1.2 - use two different HF QA models: use the default question-answering pipeline, then use other models of choice and discuss the differences in the result.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\manan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown, Latex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover The Sign of the Four by Arthur Conan Doyle Contents Chapter I. The Science of Deduction Chapter II. The Statement of the Case Chapter III. In Quest of a Solution Chapter IV. The Story of the Bald-Headed Man Chapter V. The Tragedy of Pondicherry Lodge Chapter VI. Sherlock Holmes Gives a Demonstration Chapter VII. The Episode of the Barrel Chapter VIII. The Baker Street Irregulars Chapter IX. A Break in the Chain Chapter X. The End of the Islander Chapter XI. The Great Agra Treasure Chapter XII. The Strange Story of Jonathan Small Chapter I The Science of Deduction Sherlock Holmes took his bottle from the corner of the mantel-piece and his hypodermic syringe from its neat morocco case. With his long, white, nervous fingers he adjusted the delicate needle, and rolled back his left shirt-cuff. For some little time his eyes rested thoughtfully upon the sinewy forearm and wrist all dotted and scarred with innumerable puncture-marks. Finally he thrust the sharp point home, pressed down the tiny piston, and sank back into the velvet-lined arm-chair with a long sigh of satisfaction. Three times a day for many months I had witnessed this performance, but custom had not reconciled my mind to it. On the contrary, from day to day I had become more irritable at the sight, and my conscience swelled nightly within me at the thought that I had lacked the courage to protest. Again and again I had registered a vow that I should deliver my soul upon the subject, but there was that in the cool, nonchalant air of my companion which made him the last man with whom one would care to take anything approaching to a liberty. His great powers, his masterly manner, and the experience which I had had of his many\n",
      "-----------------------\n",
      "extraordinary qualities, all made me diffident and backward in crossing him. Yet upon that afternoon, whether it was the Beaune which I had taken with my lunch, or the additional exasperation produced by the extreme deliberation of his manner, I suddenly felt that I could hold out no longer. \"Which is it to-day?\" I asked,-\"morphine or cocaine?\" He raised his eyes languidly from the old black-letter volume which he had opened. \"It is cocaine,\" he said,-\"a seven-per-cent. solution. Would you care to try it?\" \"No, indeed,\" I answered, brusquely. \"My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.\" He smiled at my vehemence. \"Perhaps you are right, Watson,\" he said. \"I suppose that its influence is physically a bad one. I find it, however, so transcendently stimulating and clarifying to the mind that its secondary action is a matter of small moment.\" \"But consider!\" I said, earnestly. \"Count the cost! Your brain may, as you say, be roused and excited, but it is a pathological and morbid process, which involves increased tissue-change and may at last leave a permanent weakness. You know, too, what a black reaction comes upon you. Surely the game is hardly worth the candle. Why should you, for a mere passing pleasure, risk the loss of those great powers with which you have been endowed? Remember that I speak not only as one comrade to another, but as a medical man to one for whose constitution he is to some extent answerable.\" He did not seem offended. On the contrary, he put his finger-tips together and leaned his elbows on the arms of his chair, like one who has a relish for conversation. \"My mind,\" he said, \"rebels at stagnation. Give me problems, give me\n",
      "-----------------------\n",
      "work, give me the most abstruse cryptogram or the most intricate analysis, and I am in my own proper atmosphere. I can dispense then with artificial stimulants. But I abhor the dull routine of existence. I crave for mental exaltation. That is why I have chosen my own particular profession,-or rather created it, for I am the only one in the world.\" \"The only unofficial detective?\" I said, raising my eyebrows. \"The only unofficial consulting detective,\" he answered. \"I am the last and highest court of appeal in detection. When Gregson or Lestrade or Athelney Jones are out of their depths-which, by the way, is their normal state-the matter is laid before me. I examine the data, as an expert, and pronounce a specialist's opinion. I claim no credit in such cases. My name figures in no newspaper. The work itself, the pleasure of finding a field for my peculiar powers, is my highest reward. But you have yourself had some experience of my methods of work in the Jefferson Hope case.\" \"Yes, indeed,\" said I, cordially. \"I was never so struck by anything in my life. I even embodied it in a small brochure with the somewhat fantastic title of 'A Study in Scarlet.'\" He shook his head sadly. \"I glanced over it,\" said he. \"Honestly, I cannot congratulate you upon it. Detection is, or ought to be, an exact science, and should be treated in the same cold and unemotional manner. You have attempted to tinge it with romanticism, which produces much the same effect as if you worked a love-story or an elopement into the fifth proposition of Euclid.\" \"But the romance was there,\" I remonstrated. \"I could not tamper with the facts.\" \"Some facts should be suppressed, or at least a just sense of proportion should\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/the_sign_of_the_four_proc.txt', 'r') as f:\n",
    "  text = f.read()\n",
    "\n",
    "chunks = []\n",
    "chunk_size = 300\n",
    "text = text.split()\n",
    "for i in range(0, len(text), chunk_size):\n",
    "  chunks.append(' '.join(text[i:i+chunk_size]))\n",
    "  # chunks.append(text)\n",
    "\n",
    "for chunk in chunks[:3]:\n",
    "  print(chunk)\n",
    "  print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144, 468])\n",
      "<class 'torch.Tensor'>\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "468\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# for chunk in chunks[:3]:\n",
    "chunk_vec = tokenizer(chunks, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "print(chunk_vec.shape)\n",
    "\n",
    "# chunks_vecs.append(chunk_vec)\n",
    "print(type(chunk_vec))\n",
    "for ch in chunk_vec:\n",
    "  print(len(ch))\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48)\n",
      "tensor(48)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "pungent a smell as this? It sounds like a sum in the rule of three. The answer should give us the-But halloa! here are the accredited representatives of the law.\" Heavy steps and the clamour of loud voices were audible from below, and the hall door shut with a loud crash. \"Before they come,\" said Holmes, \"just put your hand here on this poor fellow's arm, and here on his leg. What do you feel?\" \"The muscles are as hard as a board,\" I answered. \"Quite so. They are in a state of extreme contraction, far exceeding the usual _rigor mortis_. Coupled with this distortion of the face, this Hippocratic smile, or '_risus sardonicus_,' as the old writers called it, what conclusion would it suggest to your mind?\" \"Death from some powerful vegetable alkaloid,\" I answered,-\"some strychnine-like substance which would produce tetanus.\" \"That was the idea which occurred to me the instant I saw the drawn muscles of the face. On getting into the room I at once looked for the means by which the poison had entered the system. As you saw, I discovered a thorn which had been driven or shot with no great force into the scalp. You observe that the part struck was that which would be turned towards the hole in the ceiling if the man were erect in his chair. Now examine the thorn.\" I took it up gingerly and held it in the light of the lantern. It was long, sharp, and black, with a glazed look near the point as though some gummy substance had dried upon it. The blunt end had been trimmed and rounded off with a knife. \"Is that an English thorn?\" he asked. \"No, it certainly is not.\" \"With all these data you should be able to draw"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(0.2100)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor(68)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "fortunately, we have no distance to go. Evidently what puzzled the dog at the corner of Knight's Place was that there were two different trails running in opposite directions. We took the wrong one. It only remains to follow the other.\" There was no difficulty about this. On leading Toby to the place where he had committed his fault, he cast about in a wide circle and finally dashed off in a fresh direction. \"We must take care that he does not now bring us to the place where the creasote-barrel came from,\" I observed. \"I had thought of that. But you notice that he keeps on the pavement, whereas the barrel passed down the roadway. No, we are on the true scent now.\" It tended down towards the river-side, running through Belmont Place and Prince's Street. At the end of Broad Street it ran right down to the water's edge, where there was a small wooden wharf. Toby led us to the very edge of this, and there stood whining, looking out on the dark current beyond. \"We are out of luck,\" said Holmes. \"They have taken to a boat here.\" Several small punts and skiffs were lying about in the water and on the edge of the wharf. We took Toby round to each in turn, but, though he sniffed earnestly, he made no sign. Close to the rude landing-stage was a small brick house, with a wooden placard slung out through the second window. \"Mordecai Smith\" was printed across it in large letters, and, underneath, \"Boats to hire by the hour or day.\" A second inscription above the door informed us that a steam launch was kept,-a statement which was confirmed by a great pile of coke upon the jetty. Sherlock Holmes looked slowly round, and his"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(0.1874)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor(59)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sherlock Holmes was on the roof, and I could see him like an enormous glow-worm crawling very slowly along the ridge. I lost sight of him behind a stack of chimneys, but he presently reappeared, and then vanished once more upon the opposite side. When I made my way round there I found him seated at one of the corner eaves. \"That you, Watson?\" he cried. \"Yes.\" \"This is the place. What is that black thing down there?\" \"A water-barrel.\" \"Top on it?\" \"Yes.\" \"No sign of a ladder?\" \"No.\" \"Confound the fellow! It's a most break-neck place. I ought to be able to come down where he could climb up. The water-pipe feels pretty firm. Here goes, anyhow.\" There was a scuffling of feet, and the lantern began to come steadily down the side of the wall. Then with a light spring he came on to the barrel, and from there to the earth. \"It was easy to follow him,\" he said, drawing on his stockings and boots. \"Tiles were loosened the whole way along, and in his hurry he had dropped this. It confirms my diagnosis, as you doctors express it.\" The object which he held up to me was a small pocket or pouch woven out of coloured grasses and with a few tawdry beads strung round it. In shape and size it was not unlike a cigarette-case. Inside were half a dozen spines of dark wood, sharp at one end and rounded at the other, like that which had struck Bartholomew Sholto. \"They are hellish things,\" said he. \"Look out that you don't prick yourself. I'm delighted to have them, for the chances are that they are all he has. There is the less fear of you or me finding one in our skin before long."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(0.1860)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor(48)\n",
      "tensor([0.0198, 0.1365, 0.0083, 0.0295, 0.0875, 0.0198, 0.0173, 0.0285, 0.1534,\n",
      "        0.0127, 0.0066, 0.0265, 0.0171, 0.0265, 0.1281, 0.1009, 0.0932, 0.0331,\n",
      "        0.0151, 0.0559, 0.0175, 0.0152, 0.1116, 0.1615, 0.0185, 0.0417, 0.0199,\n",
      "        0.0174, 0.0170, 0.0395, 0.0186, 0.0716, 0.0154, 0.0173, 0.0168, 0.0213,\n",
      "        0.0551, 0.1393, 0.0168, 0.1239, 0.0173, 0.0173, 0.0283, 0.0147, 0.0195,\n",
      "        0.0310, 0.0188, 0.0152, 0.2100, 0.0133, 0.0118, 0.0107, 0.0176, 0.0166,\n",
      "        0.0140, 0.0604, 0.0161, 0.0336, 0.0140, 0.1860, 0.0076, 0.0191, 0.0834,\n",
      "        0.0175, 0.0214, 0.0484, 0.0530, 0.0178, 0.1874, 0.0340, 0.0277, 0.0192,\n",
      "        0.0224, 0.1310, 0.0136, 0.0161, 0.0157, 0.0895, 0.0196, 0.0084, 0.0176,\n",
      "        0.0573, 0.0995, 0.0164, 0.1016, 0.0450, 0.0621, 0.1394, 0.0179, 0.0216,\n",
      "        0.0164, 0.0658, 0.0260, 0.0440, 0.0137, 0.0361, 0.0808, 0.0626, 0.0185,\n",
      "        0.0155, 0.1101, 0.0412, 0.0883, 0.0728, 0.0201, 0.0646, 0.1225, 0.0167,\n",
      "        0.0454, 0.0162, 0.0169, 0.0137, 0.0197, 0.0737, 0.0567, 0.1615, 0.1329,\n",
      "        0.0828, 0.0180, 0.0243, 0.0095, 0.0018, 0.0193, 0.0104, 0.0247, 0.0573,\n",
      "        0.0175, 0.0084, 0.0082, 0.1220, 0.0167, 0.0177, 0.0198, 0.0210, 0.0177,\n",
      "        0.0425, 0.0228, 0.0464, 0.0269, 0.0611, 0.0204, 0.0191, 0.0354, 0.0175])\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "crime scene\n",
    "\"\"\"\n",
    "\n",
    "question = question.strip()\n",
    "question_vec = tokenizer(question, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "vec_len = len(chunk_vec[0])\n",
    "# print(question_vec.shape)\n",
    "# print(vec_len)\n",
    "# pad 0 to question_vec\n",
    "question_vec = torch.cat((question_vec, torch.zeros((1, vec_len - len(question_vec[0])), dtype=chunk_vec.dtype)), dim=1)\n",
    "\n",
    "# print(question_vec.shape)\n",
    "chunk_vec = chunk_vec.float()\n",
    "question_vec = question_vec.float()\n",
    "cs = torch.nn.functional.cosine_similarity(chunk_vec, question_vec, dim=1)\n",
    "print(cs.argmax(dim=0))\n",
    "# print top 3 most cs chunks\n",
    "topk = cs.topk(3)\n",
    "# print chunks at topk indices\n",
    "for i in topk.indices:\n",
    "  print(i)\n",
    "  print(display(Markdown(chunks[i])))\n",
    "  print(cs[i])\n",
    "  print('-'*100)\n",
    "\n",
    "# print first occurence of question in text based on cs\n",
    "print(cs.argmax(dim=0))\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover The Sign of the Four by Arthur Conan Doyle Contents Chapter I. The Science of Deduction Chapter II. The Statement of the Case Chapter III. In Quest of a Solution Chapter IV. The Story of the Bald-Headed Man Chapter V. The Tragedy of Pondicherry Lodge Chapter VI. Sherlock Holmes Gives a Demonstration Chapter VII. The Episode of the Barrel Chapter VIII. The Baker Street Irregulars Chapter IX. A Break in the Chain Chapter X. The End of the Islander Chapter XI. The Great Agra Treasure Chapter XII. The Strange Story of Jonathan Small Chapter I The Science of Deduction Sherlock Holmes took his bottle from the corner of the mantel-piece and his hypodermic syringe from its neat morocco case. With his long, white, nervous fingers he adjusted the delicate needle, and rolled back his left shirt-cuff. For some little time his eyes rested thoughtfully upon the sinewy forearm and wrist all dotted and scarred with innumerable puncture-marks. Finally he thrust the sharp point home, pressed down the tiny piston, and sank back into the velvet-lined arm-chair with a long sigh of satisfaction. Three times a day for many months I had witnessed this performance, but custom had not reconciled my mind to it. On the contrary, from day to day I had become more irritable at the sight, and my conscience swelled nightly within me at the thought that I had lacked the courage to protest. Again and again I had registered a vow that I should deliver my soul upon the subject, but there was that in the cool, nonchalant air of my companion which made him the last man with whom one would care to take anything approaching to a liberty. His great powers, his masterly manner, and the experience which I had had of his many\n",
      "tensor([0.1749])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "extraordinary qualities, all made me diffident and backward in crossing him. Yet upon that afternoon, whether it was the Beaune which I had taken with my lunch, or the additional exasperation produced by the extreme deliberation of his manner, I suddenly felt that I could hold out no longer. \"Which is it to-day?\" I asked,-\"morphine or cocaine?\" He raised his eyes languidly from the old black-letter volume which he had opened. \"It is cocaine,\" he said,-\"a seven-per-cent. solution. Would you care to try it?\" \"No, indeed,\" I answered, brusquely. \"My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.\" He smiled at my vehemence. \"Perhaps you are right, Watson,\" he said. \"I suppose that its influence is physically a bad one. I find it, however, so transcendently stimulating and clarifying to the mind that its secondary action is a matter of small moment.\" \"But consider!\" I said, earnestly. \"Count the cost! Your brain may, as you say, be roused and excited, but it is a pathological and morbid process, which involves increased tissue-change and may at last leave a permanent weakness. You know, too, what a black reaction comes upon you. Surely the game is hardly worth the candle. Why should you, for a mere passing pleasure, risk the loss of those great powers with which you have been endowed? Remember that I speak not only as one comrade to another, but as a medical man to one for whose constitution he is to some extent answerable.\" He did not seem offended. On the contrary, he put his finger-tips together and leaned his elbows on the arms of his chair, like one who has a relish for conversation. \"My mind,\" he said, \"rebels at stagnation. Give me problems, give me\n",
      "tensor([0.0833])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "work, give me the most abstruse cryptogram or the most intricate analysis, and I am in my own proper atmosphere. I can dispense then with artificial stimulants. But I abhor the dull routine of existence. I crave for mental exaltation. That is why I have chosen my own particular profession,-or rather created it, for I am the only one in the world.\" \"The only unofficial detective?\" I said, raising my eyebrows. \"The only unofficial consulting detective,\" he answered. \"I am the last and highest court of appeal in detection. When Gregson or Lestrade or Athelney Jones are out of their depths-which, by the way, is their normal state-the matter is laid before me. I examine the data, as an expert, and pronounce a specialist's opinion. I claim no credit in such cases. My name figures in no newspaper. The work itself, the pleasure of finding a field for my peculiar powers, is my highest reward. But you have yourself had some experience of my methods of work in the Jefferson Hope case.\" \"Yes, indeed,\" said I, cordially. \"I was never so struck by anything in my life. I even embodied it in a small brochure with the somewhat fantastic title of 'A Study in Scarlet.'\" He shook his head sadly. \"I glanced over it,\" said he. \"Honestly, I cannot congratulate you upon it. Detection is, or ought to be, an exact science, and should be treated in the same cold and unemotional manner. You have attempted to tinge it with romanticism, which produces much the same effect as if you worked a love-story or an elopement into the fifth proposition of Euclid.\" \"But the romance was there,\" I remonstrated. \"I could not tamper with the facts.\" \"Some facts should be suppressed, or at least a just sense of proportion should\n",
      "tensor([0.0751])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "be observed in treating them. The only point in the case which deserved mention was the curious analytical reasoning from effects to causes by which I succeeded in unraveling it.\" I was annoyed at this criticism of a work which had been specially designed to please him. I confess, too, that I was irritated by the egotism which seemed to demand that every line of my pamphlet should be devoted to his own special doings. More than once during the years that I had lived with him in Baker Street I had observed that a small vanity underlay my companion's quiet and didactic manner. I made no remark, however, but sat nursing my wounded leg. I had a Jezail bullet through it some time before, and, though it did not prevent me from walking, it ached wearily at every change of the weather. \"My practice has extended recently to the Continent,\" said Holmes, after a while, filling up his old brier-root pipe. \"I was consulted last week by Francois Le Villard, who, as you probably know, has come rather to the front lately in the French detective service. He has all the Celtic power of quick intuition, but he is deficient in the wide range of exact knowledge which is essential to the higher developments of his art. The case was concerned with a will, and possessed some features of interest. I was able to refer him to two parallel cases, the one at Riga in 1857, and the other at St. Louis in 1871, which have suggested to him the true solution. Here is the letter which I had this morning acknowledging my assistance.\" He tossed over, as he spoke, a crumpled sheet of foreign notepaper. I glanced my eyes down it, catching a profusion of notes of admiration,\n",
      "tensor([0.0576])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "with stray \"magnifiques,\" \"coup-de-maitres,\" and \"tours-de-force,\" all testifying to the ardent admiration of the Frenchman. \"He speaks as a pupil to his master,\" said I. \"Oh, he rates my assistance too highly,\" said Sherlock Holmes, lightly. \"He has considerable gifts himself. He possesses two out of the three qualities necessary for the ideal detective. He has the power of observation and that of deduction. He is only wanting in knowledge; and that may come in time. He is now translating my small works into French.\" \"Your works?\" \"Oh, didn't you know?\" he cried, laughing. \"Yes, I have been guilty of several monographs. They are all upon technical subjects. Here, for example, is one 'Upon the Distinction between the Ashes of the Various Tobaccoes.' In it I enumerate a hundred and forty forms of cigar-, cigarette-, and pipe-tobacco, with coloured plates illustrating the difference in the ash. It is a point which is continually turning up in criminal trials, and which is sometimes of supreme importance as a clue. If you can say definitely, for example, that some murder has been done by a man who was smoking an Indian lunkah, it obviously narrows your field of search. To the trained eye there is as much difference between the black ash of a Trichinopoly and the white fluff of bird's-eye as there is between a cabbage and a potato.\" \"You have an extraordinary genius for minutiae,\" I remarked. \"I appreciate their importance. Here is my monograph upon the tracing of footsteps, with some remarks upon the uses of plaster of Paris as a preserver of impresses. Here, too, is a curious little work upon the influence of a trade upon the form of the hand, with lithotypes of the hands of slaters, sailors, corkcutters, compositors, weavers, and diamond-polishers. That is a\n",
      "tensor([0.1432])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Interface\n",
    "\n",
    "For a prompt, Implement a simple prompt interface that takes in your question, runs it against the model, and returns the answer.\n",
    "\n",
    "You don't need to do anything special about this, just a simple console I/O interface without any complicated error handling.\n",
    "\n",
    "It is up to you how you want to upload the context to the model (pre-loaded into your program, on-demand, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    model: Optional[str] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"model: {model}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"~\" * 80)\n",
    "\n",
    "    # Construct Pipeline\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        # model=\"deepset/roberta-base-squad2\",\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Run Pipeline\n",
    "\n",
    "    question = question.strip()\n",
    "    context = context.strip()\n",
    "\n",
    "    print(f\"C: {context}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    # display(Markdown(f\"**Q:** {question}\"))\n",
    "    # display(Markdown(f\"**C:** {context}\"))\n",
    "\n",
    "    res = pipe(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # pprint(res)\n",
    "\n",
    "    answer, score = \"idk\", 1.0\n",
    "\n",
    "    # Get the result\n",
    "    if res and isinstance(res, dict):\n",
    "        answer = res.get(\"answer\", \"idk\")\n",
    "        score = res.get(\"score\", 1.0)\n",
    "\n",
    "    answer = answer.strip()\n",
    "    score = round(score, 3)\n",
    "\n",
    "    print(f\"A: {answer} (score: {round(score, 3)})\")\n",
    "    # display(Markdown(f\"**A:** {answer} (score: {score})\"))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def run_models(question: str, context: str, models: list[str], **kwargs):\n",
    "    for model in models:\n",
    "        run(question, context, model=model, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Prompt Interface\n",
    "\n",
    "Models: https://huggingface.co/models?pipeline_tag=question-answering&sort=trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ctx = \"\"\"\n",
    "Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
    "his hypodermic syringe from its neat morocco case. With his long,\n",
    "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
    "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
    "upon the sinewy forearm and wrist all dotted and scarred with\n",
    "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
    "pressed down the tiny piston, and sank back into the velvet-lined\n",
    "arm-chair with a long sigh of satisfaction.\n",
    "\"\"\"\n",
    "\n",
    "test_q = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: None\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "Q: What's my name?\n",
      "A: Sherlock Holmes (score: 0.761)\n"
     ]
    }
   ],
   "source": [
    "# Default - \"distilbert-base-uncased-distilled-squad\"\n",
    "_ = run(test_q, test_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "c:\\Users\\manan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 496M/496M [01:10<00:00, 7.05MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 79.0/79.0 [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 6.74MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 6.77MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "Q: What's my name?\n",
      "A: Sherlock Holmes (score: 0.114)\n"
     ]
    }
   ],
   "source": [
    "_ = run(test_q, test_ctx, model=\"deepset/roberta-base-squad2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 451/451 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|██████████| 265M/265M [00:33<00:00, 7.87MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 7.25MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 7.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "Q: What's my name?\n",
      "A: Sherlock Holmes (score: 0.99)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: Sherlock Holmes took his bottle from the corner of the mantel-piece and\n",
      "his hypodermic syringe from its neat morocco case. With his long,\n",
      "white, nervous fingers he adjusted the delicate needle, and rolled back\n",
      "his left shirt-cuff. For some little time his eyes rested thoughtfully\n",
      "upon the sinewy forearm and wrist all dotted and scarred with\n",
      "innumerable puncture-marks. Finally he thrust the sharp point home,\n",
      "pressed down the tiny piston, and sank back into the velvet-lined\n",
      "arm-chair with a long sigh of satisfaction.\n",
      "Q: What's my name?\n",
      "A: Sherlock Holmes (score: 0.114)\n"
     ]
    }
   ],
   "source": [
    "run_models(\n",
    "    test_q,\n",
    "    test_ctx,\n",
    "    models=[\n",
    "        \"distilbert-base-uncased-distilled-squad\",\n",
    "        \"deepset/roberta-base-squad2\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments & Results\n",
    "\n",
    "For the first part, use the Hugging Face question-answering pipeline and feed it with the five 300-word long sections from the book of your choice that you analyzed in Project 1.\n",
    "\n",
    "These sections should be selected so they are: **introducing the protagonist(s), the antagonist, the crime and crime scene, any significant evidence, and the resolution of the crime/a narrative that presents the case against the perpetrator.**\n",
    "\n",
    "The questions you should ask are about the identity and characteristics of the protagonist, antagonist/perpetrator, the nature and the setting of the crime or crime scene, the evidence, and the case against the perpetrator.\n",
    "\n",
    "Document the questions, ask the questions, and document the specificity and accuracy of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out a good selection of models and keep some interesting ones\n",
    "models = [\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"\"\"\n",
    "My name Jeff.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.987)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "s1q1 = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n",
    "\n",
    "run_models(s1q1, s1, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more cells and ask more questions on Section 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: document the specificity and accuracy of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = \"\"\"\n",
    "My name Jeff.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.987)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "s2q1 = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n",
    "\n",
    "run_models(s2q1, s2, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more cells and ask more questions on Section 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: document the specificity and accuracy of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = \"\"\"\n",
    "My name Jeff.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.987)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "s3q1 = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "run_models(s3q1, s3, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more cells and ask more questions on Section 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: document the specificity and accuracy of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = \"\"\"\n",
    "My name Jeff.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.987)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "s4q1 = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "run_models(s4q1, s4, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more cells and ask more questions on Section 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: document the specificity and accuracy of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = \"\"\"\n",
    "My name Jeff.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "model: distilbert-base-uncased-distilled-squad\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.987)\n",
      "====================================================================================================\n",
      "model: deepset/roberta-base-squad2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "C: My name Jeff.\n",
      "Q: What's my name?\n",
      "A: Jeff (score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "s5q1 = \"\"\"\n",
    "What's my name?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "run_models(s5q1, s5, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more cells and ask more questions on Section 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: document the specificity and accuracy of the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
